{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from bson.objectid import ObjectId\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# MongoDB connection details\n",
    "MONGO_URI = \"\"\n",
    "DB_NAME = \"\"\n",
    "COLLECTION_NAME = \"\"\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = pymongo.MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "print(\"Connected to MongoDB:\")\n",
    "\n",
    "# Load the sentence-transformer model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Loaded sentence-transformer model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path: str) -> Dict:\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def split_large_elements(data: List[str], max_length: int = 512) -> List[str]:\n",
    "    \"\"\"Split large elements into smaller chunks based on character count.\"\"\"\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if len(item) > max_length:\n",
    "            chunks = [item[i:i + max_length] for i in range(0, len(item), max_length)]\n",
    "            result.extend(chunks)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def create_vector_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Create a vector embedding using the sentence-transformer model.\"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()\n",
    "\n",
    "def process_json_data(data: Dict) -> List[Dict]:\n",
    "    \"\"\"Process JSON data to create embeddings for each key's array elements.\"\"\"\n",
    "    processed_data = []\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, list):\n",
    "            value = [str(item) for item in value]\n",
    "            value = split_large_elements(value)\n",
    "            for idx, element in enumerate(value):\n",
    "                embedding = create_vector_embedding(element)\n",
    "                processed_data.append({\n",
    "                    \"text\": element,\n",
    "                    \"embedding\": embedding,\n",
    "                    \"key\": key,\n",
    "                    \"index\": idx\n",
    "                })\n",
    "    return processed_data\n",
    "\n",
    "def split_large_elements2(data: str, max_length: int = 512) -> List[str]:\n",
    "    \"\"\"Split large elements into smaller chunks based on character count.\"\"\"\n",
    "    result = []\n",
    "    if len(data) > max_length:\n",
    "        chunks = [data[i:i + max_length] for i in range(0, len(data), max_length)]\n",
    "        result.extend(chunks)\n",
    "    else:\n",
    "        result.append(data)\n",
    "    return result\n",
    "\n",
    "def process_collection_data(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Process collection data to create embeddings for each document.\"\"\"\n",
    "    processed_data = []\n",
    "    for doc in tqdm(data):\n",
    "        str_doc = doc.get(\"text\", str(doc))\n",
    "        value = split_large_elements2(str_doc)\n",
    "        \n",
    "        for idx, element in enumerate(value):\n",
    "            embedding = create_vector_embedding(element)  \n",
    "            processed_data.append({\n",
    "                \"text\": element,\n",
    "                \"embedding\": embedding, \n",
    "                \"key\": doc.get(\"label\"),\n",
    "                \"index\": idx \n",
    "            })\n",
    "    return processed_data\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from typing import List, Dict\n",
    "\n",
    "def sync_with_mongodb(new_data: List[Dict]):\n",
    "    \"\"\"Directly dump new data to MongoDB.\"\"\"\n",
    "    # Insert all new data into MongoDB\n",
    "    if new_data:\n",
    "        collection.insert_many(new_data)\n",
    "        print(f\"Inserted {len(new_data)} new records into the database.\")\n",
    "    else:\n",
    "        print(\"No new data to insert.\")\n",
    "\n",
    "\n",
    "def task1(json_file_path: str):\n",
    "    \"\"\"Main function to load JSON data, create embeddings, and sync with MongoDB.\"\"\"\n",
    "    data = load_json_data(json_file_path)\n",
    "    print(\"Loaded Data:\", data)\n",
    "    print(data.items())\n",
    "    processed_data = process_json_data(data)\n",
    "    print(\"Processed Data:\", processed_data)\n",
    "    sync_with_mongodb(processed_data)\n",
    "    \n",
    "def run_data_aggr(collection):\n",
    "    \"\"\"Aggregate project data with associated articles.\"\"\"\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$lookup\": {\n",
    "                \"from\": \"article\",\n",
    "                \"localField\": \"_id\",\n",
    "                \"foreignField\": \"projectId\",\n",
    "                \"as\": \"articles\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    return list(collection.aggregate(pipeline))\n",
    "    \n",
    "    \n",
    "def task2():\n",
    "    \"\"\"Main function to load data from project and article collections and sync with MongoDB vector store.\"\"\"\n",
    "    MONGO_URI2 = \"\"\n",
    "    DB_NAME2 = \"\"\n",
    "    COLLECTION_NAME2 = \"\"\n",
    "\n",
    "    client = pymongo.MongoClient(MONGO_URI2)\n",
    "    db = client[DB_NAME2]\n",
    "    collection = db[COLLECTION_NAME2]\n",
    "    print(\"Connected to MongoDB:\")\n",
    "    \n",
    "    raw_data = run_data_aggr(collection)\n",
    "    \n",
    "    processed_data = process_collection_data(raw_data)\n",
    "    print(\"Processed Data:\", processed_data)\n",
    "    sync_with_mongodb(processed_data)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    json_file_path = r\"C:\\Users\\anant\\Desktop\\website repo\\projects playground\\about_me_llm\\data_about_me_2.json\"  # Replace with your JSON file path\n",
    "    task1(json_file_path)\n",
    "    task2()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3149ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "        collection=collection,\n",
    "        embedding=embeddings,\n",
    "        text_key=\"text\",\n",
    "        embedding_key=\"embedding\",\n",
    "        index_name=\"default\",\n",
    "        relevance_score_fn = \"cosine\" # Replace with your index name\n",
    "        )\n",
    "results = vector_store.similarity_search(query = \"Tell me about ananths work experience\", k=10)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe825cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d02f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e49fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12332856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441931b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69240515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd5ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
